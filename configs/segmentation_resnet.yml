---
model: unet
dataset: custom_semantic_segmentation
path_dataset: T:\trainingdata\befastelse\debugset\data\splitted
path_labels: T:\trainingdata\befastelse\debugset\labels\splitted_labels
data_sources: [rgb,cir,OrtoRGB,OrtoCIR,DSM,DTM]
channels: [[0,1,2],[0],[0,1,2],[0],[0],[0]]
#data_sources: [rgb,cir]
#channels: [[0,1,2],[0]]
n_input_channels: 10
all_txt: T:\trainingdata\befastelse\debugset\data\all.txt
valid_txt: T:\trainingdata\befastelse\debugset\data\valid.txt
train_txt: T:\trainingdata\befastelse\debugset\data\train.txt

optimizer: adam
wd: 0.0
ignore_index: -100
label_smoothing: 0.0001
epochs: 10
lr: 0.1
learning_rate_schedule:
 name: "LinearLR"
batchsize: 4
accumulate_grad_batches: 4
gradient_clip_val: 1.0
save_interval: 100
save_dir: "mcocologs/resnet34/"
resume_training: False
val_check_interval: 1
log_every_n_steps: 550
accelerator: auto
state_dict_path_load: False
experment_name: "debugset_renset34"
loss: cross_entropy


#normalization
#The datatye of the input affects how differetn transforms behave. floats will be cipped to range[-1,1].
#It is sometimes necessary to cast the input images to uin8 in order to make sure this format is used
#OBS: lidar values are floats and these datatypes might have to be preprocessed in some better way!
convert_input_data_to: uint8


means: [0.37717373,0.36551262,0.35784693,0.21209801,0.4461955,0.43208109,0.4201469,0.47909955,0.11747267,0.09866089]
stds: [0.16392524,0.1445057,0.134715,0.13147732,0.17134316,0.15107451,0.1288624,0.1815257,0.02500286,0.00640732]

#means: [0.485, 0.456, 0.406,0.40779021]
#stds: [0.229, 0.224, 0.225,0.15176421]


#pytorch lightning specific settings
#https://lightning.ai/docs/pytorch/latest/common/trainer.html#num-sanity-val-steps
num_sanity_val_steps: 0


#visaulization
#set this to a positive number in order to visualize the data as it looks after transforms have been aplied
nr_of_visualizations: 0


