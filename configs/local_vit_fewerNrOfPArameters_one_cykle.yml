---
model: vit_local
keep_nr_of_paramters_equal: False
dataset: imagenet1k
path_dataset: False
optimizer: adam
# 'exclude the parameters of the LayerNorm for weight decay. 
#Or you can just turn off weight decay, it usually makes minor difference
#'https://github.com/lucidrains/vit-pytorch/issues/55
wd: 0.00
ignore_index: -100
label_smoothing: 0.0001
#in  https://github.com/google-research/vision_transformer/issues/89 they propose batchsize of 4096 ==(256*16) and warmup for 10000 steps. (1281167 images / 4096 == 312 steps per epoch  --> 10000 steps == 32 epochs)
#with a planned trainingtime of 300 epochs pct_start == 32/300 == 0.106 
epochs: 300

lr: 0.001
learning_rate_schedule:
 name: fit_one_cykle
 pct_start: 0.106
batchsize: 128
accumulate_grad_batches: 32
#gradient norml clip should be 1. acoridng to 'how to train your vit'
gradient_clip_val: 1.0
save_interval: 100
save_dir: "logs_localViTFewerNrofParameters_one_cykle/"
val_check_interval: 1
num_sanity_val_steps: 0
overfit_batches: False
log_every_n_steps: 550
accelerator: auto
state_dict_path_load: False
resume_training: False
experment_name: "local ViT fewer parameters one cykle on imagenet1k"
loss: cross_entropy
